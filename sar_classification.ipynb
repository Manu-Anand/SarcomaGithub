{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HHHHHHH\n",
      "['Synovial Sarcoma', 'Undifferentiated Sarcoma', 'dedifferentiated liposarcoma', 'leiomyosarcoma (lms)', 'malignant peripheral nerve sheath tumors (mpnst)', 'myxofibrosarcoma']\n",
      "     cg00000165  cg00000236  cg00000289  cg00000292  cg00000321  cg00000363  \\\n",
      "0      0.822568    0.922048    0.750420    0.377120    0.069381    0.805901   \n",
      "1      0.235924    0.906233    0.612593    0.922852    0.089916    0.251482   \n",
      "2      0.814205    0.895598    0.617794    0.752132    0.746498    0.410345   \n",
      "3      0.416769    0.893083    0.656475    0.668648    0.528341    0.399737   \n",
      "4      0.136909    0.812579    0.624955    0.666994    0.330979    0.120983   \n",
      "5      0.705386    0.887615    0.569937    0.796094    0.506325    0.273498   \n",
      "6      0.503227    0.905776    0.616173    0.421955    0.171419    0.238287   \n",
      "7      0.187319    0.872477    0.611403    0.766338    0.410168    0.368802   \n",
      "8      0.243633    0.837798    0.565070    0.818292    0.234684    0.093653   \n",
      "9      0.114265    0.897449    0.575567    0.351752    0.770525    0.530254   \n",
      "10     0.398753    0.869982    0.617263    0.362125    0.262385    0.404551   \n",
      "11     0.111883    0.911231    0.606715    0.656812    0.110444    0.057615   \n",
      "12     0.893571    0.891597    0.681003    0.759931    0.405947    0.217996   \n",
      "13     0.097898    0.777402    0.739334    0.446952    0.392147    0.124216   \n",
      "14     0.402016    0.903251    0.623041    0.806413    0.536773    0.311257   \n",
      "15     0.394922    0.853934    0.790487    0.822571    0.351903    0.406137   \n",
      "16     0.094590    0.935500    0.621828    0.300741    0.809565    0.190939   \n",
      "17     0.526392    0.923358    0.733617    0.335851    0.854889    0.629503   \n",
      "18     0.088467    0.889243    0.688584    0.639180    0.158804    0.237757   \n",
      "19     0.845032    0.872888    0.475089    0.793910    0.215187    0.135823   \n",
      "20     0.786804    0.852900    0.494441    0.506421    0.845432    0.251785   \n",
      "21     0.302585    0.913170    0.460924    0.443258    0.260863    0.413732   \n",
      "22     0.083444    0.920060    0.584836    0.893278    0.571271    0.416695   \n",
      "23     0.102185    0.901786    0.642481    0.797016    0.400669    0.522437   \n",
      "24     0.625584    0.750050    0.614915    0.499946    0.633791    0.302144   \n",
      "25     0.767456    0.908518    0.811701    0.907380    0.162193    0.530788   \n",
      "26     0.869933    0.774554    0.422179    0.926933    0.605406    0.837815   \n",
      "27     0.112896    0.877802    0.628864    0.749730    0.817969    0.241534   \n",
      "28     0.277806    0.912631    0.676126    0.827969    0.638925    0.626842   \n",
      "29     0.315348    0.896433    0.734562    0.158294    0.368184    0.085253   \n",
      "..          ...         ...         ...         ...         ...         ...   \n",
      "227    0.131857    0.750583    0.600585    0.821427    0.590972    0.177692   \n",
      "228    0.083998    0.942537    0.792232    0.869536    0.844026    0.773543   \n",
      "229    0.313284    0.870679    0.500187    0.542920    0.496072    0.231755   \n",
      "230    0.730595    0.908277    0.717265    0.624391    0.563129    0.271096   \n",
      "231    0.678896    0.905169    0.770184    0.880612    0.254450    0.323804   \n",
      "232    0.314634    0.926400    0.761179    0.461538    0.071089    0.399174   \n",
      "233    0.097009    0.913248    0.718115    0.876816    0.518765    0.514658   \n",
      "234    0.755567    0.918069    0.511800    0.870626    0.568453    0.241394   \n",
      "235    0.254923    0.831073    0.385360    0.508148    0.812276    0.139018   \n",
      "236    0.254575    0.887682    0.710504    0.461849    0.507340    0.215808   \n",
      "237    0.062649    0.883257    0.733043    0.664863    0.787182    0.882435   \n",
      "238    0.061936    0.832187    0.687129    0.602155    0.849627    0.102029   \n",
      "239    0.905965    0.905924    0.558882    0.346764    0.596653    0.270533   \n",
      "240    0.135599    0.860874    0.788390    0.781368    0.237311    0.314651   \n",
      "241    0.552867    0.906367    0.704282    0.892059    0.590994    0.090045   \n",
      "242    0.410689    0.858653    0.503175    0.190493    0.168413    0.160024   \n",
      "243    0.874554    0.878812    0.597552    0.665323    0.829138    0.143469   \n",
      "244    0.236504    0.871048    0.614432    0.560653    0.743371    0.189915   \n",
      "245    0.094885    0.870495    0.776033    0.721201    0.216538    0.510646   \n",
      "246    0.103761    0.927552    0.742993    0.932381    0.733691    0.500182   \n",
      "247    0.165617    0.882684    0.497082    0.503169    0.170103    0.140715   \n",
      "248    0.095291    0.853955    0.617088    0.129088    0.084845    0.065976   \n",
      "249    0.420702    0.841735    0.762498    0.548550    0.414018    0.332472   \n",
      "250    0.058617    0.883994    0.720902    0.227741    0.051163    0.083455   \n",
      "251    0.053760    0.852967    0.711381    0.144842    0.058107    0.060212   \n",
      "252    0.631603    0.927025    0.720999    0.918562    0.719934    0.551111   \n",
      "253    0.431553    0.941053    0.737591    0.836898    0.783336    0.404994   \n",
      "254    0.308808    0.867892    0.443950    0.231707    0.305505    0.086600   \n",
      "255    0.224773    0.681959    0.704688    0.421305    0.274722    0.269513   \n",
      "256    0.295243    0.607783    0.550484    0.807200    0.523667    0.132208   \n",
      "\n",
      "     cg00000622  cg00000658  cg00000714  cg00000721      ...       \\\n",
      "0      0.016408    0.833157    0.080073    0.942750      ...        \n",
      "1      0.016707    0.880560    0.066258    0.947538      ...        \n",
      "2      0.012328    0.883818    0.094511    0.936120      ...        \n",
      "3      0.011044    0.919343    0.120579    0.944506      ...        \n",
      "4      0.014495    0.903577    0.101141    0.941466      ...        \n",
      "5      0.011177    0.919005    0.120614    0.918397      ...        \n",
      "6      0.013056    0.845615    0.128163    0.943795      ...        \n",
      "7      0.016399    0.901225    0.189453    0.942113      ...        \n",
      "8      0.011958    0.893889    0.055309    0.941380      ...        \n",
      "9      0.012183    0.845648    0.073378    0.943660      ...        \n",
      "10     0.010854    0.858017    0.147638    0.942659      ...        \n",
      "11     0.013082    0.812997    0.052937    0.873755      ...        \n",
      "12     0.015715    0.909575    0.154338    0.949855      ...        \n",
      "13     0.017512    0.858236    0.111762    0.956870      ...        \n",
      "14     0.012649    0.831535    0.153359    0.917404      ...        \n",
      "15     0.016294    0.849476    0.200647    0.933875      ...        \n",
      "16     0.011916    0.851745    0.053133    0.928371      ...        \n",
      "17     0.013421    0.874669    0.069952    0.946406      ...        \n",
      "18     0.015454    0.800940    0.085812    0.883805      ...        \n",
      "19     0.011578    0.912099    0.101707    0.954713      ...        \n",
      "20     0.010978    0.775872    0.072667    0.926904      ...        \n",
      "21     0.014888    0.914875    0.105445    0.948177      ...        \n",
      "22     0.014287    0.909679    0.074554    0.935853      ...        \n",
      "23     0.016804    0.869911    0.115755    0.930591      ...        \n",
      "24     0.011214    0.811324    0.070137    0.930766      ...        \n",
      "25     0.011528    0.847762    0.155083    0.934172      ...        \n",
      "26     0.016649    0.803209    0.098256    0.782108      ...        \n",
      "27     0.017506    0.839623    0.087380    0.943432      ...        \n",
      "28     0.011880    0.838564    0.269264    0.940975      ...        \n",
      "29     0.011987    0.867594    0.055067    0.797055      ...        \n",
      "..          ...         ...         ...         ...      ...        \n",
      "227    0.012987    0.872481    0.081197    0.932329      ...        \n",
      "228    0.014555    0.802427    0.048586    0.942065      ...        \n",
      "229    0.014248    0.876708    0.089679    0.945011      ...        \n",
      "230    0.014538    0.850186    0.084236    0.932072      ...        \n",
      "231    0.014788    0.889890    0.071171    0.943845      ...        \n",
      "232    0.012604    0.790056    0.070544    0.939992      ...        \n",
      "233    0.017254    0.804845    0.071471    0.944688      ...        \n",
      "234    0.011510    0.921041    0.145798    0.947173      ...        \n",
      "235    0.008186    0.804454    0.106687    0.915672      ...        \n",
      "236    0.015036    0.868882    0.089955    0.943954      ...        \n",
      "237    0.012891    0.849854    0.047871    0.927388      ...        \n",
      "238    0.010521    0.862191    0.059770    0.705791      ...        \n",
      "239    0.012995    0.843124    0.062810    0.944163      ...        \n",
      "240    0.012578    0.873218    0.174426    0.936613      ...        \n",
      "241    0.014329    0.914182    0.077014    0.937212      ...        \n",
      "242    0.013519    0.882048    0.056248    0.911643      ...        \n",
      "243    0.013328    0.927253    0.056110    0.903789      ...        \n",
      "244    0.012960    0.789779    0.105071    0.913809      ...        \n",
      "245    0.016281    0.821032    0.109140    0.947143      ...        \n",
      "246    0.014505    0.913274    0.314820    0.959716      ...        \n",
      "247    0.013462    0.894802    0.102070    0.937773      ...        \n",
      "248    0.012304    0.871806    0.053420    0.901615      ...        \n",
      "249    0.015586    0.857883    0.069009    0.939969      ...        \n",
      "250    0.016396    0.895635    0.062645    0.943632      ...        \n",
      "251    0.014132    0.850657    0.057836    0.924117      ...        \n",
      "252    0.014355    0.824364    0.164870    0.917011      ...        \n",
      "253    0.011766    0.903330    0.144830    0.934633      ...        \n",
      "254    0.009823    0.880464    0.045962    0.928295      ...        \n",
      "255    0.011981    0.877463    0.140301    0.953206      ...        \n",
      "256    0.014009    0.881783    0.058035    0.850291      ...        \n",
      "\n",
      "     ch.9.84078312F  ch.9.86947500F  ch.9.898515R  ch.9.90287778F  \\\n",
      "0          0.040024        0.032365      0.035668        0.030603   \n",
      "1          0.018145        0.026842      0.045947        0.027698   \n",
      "2          0.024321        0.025752      0.039559        0.029038   \n",
      "3          0.030089        0.020795      0.028826        0.016888   \n",
      "4          0.127553        0.038898      0.044039        0.022060   \n",
      "5          0.042781        0.039074      0.094951        0.066509   \n",
      "6          0.030981        0.028066      0.040387        0.031682   \n",
      "7          0.032388        0.035352      0.059786        0.036253   \n",
      "8          0.035548        0.051021      0.060737        0.032594   \n",
      "9          0.033489        0.029629      0.056172        0.040184   \n",
      "10         0.024504        0.019581      0.067815        0.030078   \n",
      "11         0.021994        0.036558      0.046639        0.037996   \n",
      "12         0.046164        0.033050      0.034767        0.017667   \n",
      "13         0.023905        0.026512      0.030407        0.027068   \n",
      "14         0.026363        0.027942      0.059576        0.047548   \n",
      "15         0.025794        0.074349      0.037652        0.029974   \n",
      "16         0.020647        0.029255      0.047859        0.037056   \n",
      "17         0.030219        0.058655      0.069193        0.041550   \n",
      "18         0.048988        0.025969      0.065679        0.045920   \n",
      "19         0.020592        0.031930      0.078276        0.038434   \n",
      "20         0.047124        0.030067      0.053286        0.032011   \n",
      "21         0.046903        0.029344      0.063971        0.028502   \n",
      "22         0.020630        0.029863      0.036911        0.030055   \n",
      "23         0.025718        0.023395      0.042583        0.033227   \n",
      "24         0.048770        0.042944      0.050380        0.034457   \n",
      "25         0.034140        0.027312      0.046227        0.027667   \n",
      "26         0.024493        0.055086      0.100844        0.084582   \n",
      "27         0.025292        0.036333      0.038518        0.022225   \n",
      "28         0.036428        0.029254      0.039460        0.025135   \n",
      "29         0.017897        0.031496      0.042738        0.028979   \n",
      "..              ...             ...           ...             ...   \n",
      "227        0.022085        0.030804      0.037861        0.033809   \n",
      "228        0.029380        0.038257      0.030159        0.021981   \n",
      "229        0.073279        0.056202      0.105491        0.032100   \n",
      "230        0.025211        0.036144      0.049326        0.030716   \n",
      "231        0.021516        0.042193      0.030175        0.027021   \n",
      "232        0.023687        0.028045      0.046868        0.031920   \n",
      "233        0.035919        0.040167      0.050181        0.046136   \n",
      "234        0.031999        0.033258      0.061894        0.064716   \n",
      "235        0.028014        0.026711      0.074375        0.036057   \n",
      "236        0.150644        0.089749      0.059826        0.047744   \n",
      "237        0.021952        0.032030      0.048190        0.032935   \n",
      "238        0.032063        0.039595      0.088103        0.058968   \n",
      "239        0.039814        0.031338      0.021253        0.038704   \n",
      "240        0.033087        0.029748      0.041203        0.030367   \n",
      "241        0.020801        0.032344      0.036341        0.026563   \n",
      "242        0.025193        0.036704      0.039171        0.029811   \n",
      "243        0.080379        0.054830      0.057108        0.037388   \n",
      "244        0.161456        0.038508      0.081996        0.039500   \n",
      "245        0.024583        0.039178      0.045868        0.042140   \n",
      "246        0.023668        0.022297      0.047843        0.026359   \n",
      "247        0.022089        0.029856      0.040379        0.020622   \n",
      "248        0.043167        0.040841      0.061092        0.045185   \n",
      "249        0.017129        0.025375      0.026276        0.020331   \n",
      "250        0.042435        0.044918      0.046931        0.027647   \n",
      "251        0.060977        0.050570      0.034427        0.027696   \n",
      "252        0.023242        0.030387      0.031735        0.022426   \n",
      "253        0.026015        0.026156      0.034216        0.029604   \n",
      "254        0.018090        0.014410      0.035574        0.018665   \n",
      "255        0.027640        0.042885      0.070319        0.037958   \n",
      "256        0.027534        0.074749      0.029057        0.021445   \n",
      "\n",
      "     ch.9.90621653R  ch.9.93373462R  ch.9.945770F  ch.9.98957343R  \\\n",
      "0          0.026538        0.015599      0.032262        0.041088   \n",
      "1          0.017217        0.017724      0.036449        0.038383   \n",
      "2          0.026768        0.012994      0.033918        0.039117   \n",
      "3          0.018578        0.010627      0.018536        0.018257   \n",
      "4          0.029990        0.013875      0.035003        0.030309   \n",
      "5          0.029626        0.012772      0.052867        0.034562   \n",
      "6          0.025882        0.018399      0.040542        0.030533   \n",
      "7          0.023578        0.013719      0.045808        0.045960   \n",
      "8          0.034237        0.017528      0.057279        0.034395   \n",
      "9          0.029892        0.013747      0.035881        0.030584   \n",
      "10         0.021309        0.014309      0.041577        0.025760   \n",
      "11         0.024464        0.015971      0.041651        0.051844   \n",
      "12         0.029821        0.010409      0.023008        0.024144   \n",
      "13         0.020780        0.015453      0.024853        0.042026   \n",
      "14         0.026906        0.015218      0.059539        0.029651   \n",
      "15         0.024269        0.016225      0.032081        0.045444   \n",
      "16         0.038353        0.013804      0.038547        0.037959   \n",
      "17         0.035792        0.014461      0.058494        0.041085   \n",
      "18         0.023617        0.012987      0.063832        0.034982   \n",
      "19         0.025293        0.018139      0.039086        0.057395   \n",
      "20         0.025613        0.024238      0.035098        0.023875   \n",
      "21         0.022132        0.013289      0.038459        0.035204   \n",
      "22         0.022702        0.015409      0.036486        0.031887   \n",
      "23         0.017003        0.014703      0.034438        0.036073   \n",
      "24         0.051837        0.012460      0.043642        0.028839   \n",
      "25         0.019272        0.012311      0.031984        0.029792   \n",
      "26         0.026714        0.018118      0.152290        0.024945   \n",
      "27         0.027674        0.013179      0.029692        0.030846   \n",
      "28         0.022622        0.018246      0.014712        0.037268   \n",
      "29         0.022459        0.015244      0.038645        0.034989   \n",
      "..              ...             ...           ...             ...   \n",
      "227        0.024223        0.012927      0.041025        0.034705   \n",
      "228        0.025029        0.013775      0.039736        0.038976   \n",
      "229        0.063346        0.013002      0.054417        0.030853   \n",
      "230        0.021128        0.015933      0.038016        0.041781   \n",
      "231        0.020977        0.015244      0.033687        0.030702   \n",
      "232        0.024379        0.014838      0.045298        0.029044   \n",
      "233        0.037875        0.017896      0.059674        0.047711   \n",
      "234        0.021807        0.014187      0.045630        0.034957   \n",
      "235        0.018942        0.018790      0.057471        0.028657   \n",
      "236        0.040562        0.030145      0.186312        0.037492   \n",
      "237        0.020408        0.015103      0.038246        0.037326   \n",
      "238        0.030931        0.017950      0.069202        0.037745   \n",
      "239        0.031142        0.015892      0.025823        0.035641   \n",
      "240        0.024448        0.012857      0.040694        0.030266   \n",
      "241        0.022230        0.014041      0.036153        0.043928   \n",
      "242        0.030120        0.013204      0.032448        0.041143   \n",
      "243        0.075762        0.014018      0.037083        0.027676   \n",
      "244        0.030030        0.013741      0.055509        0.034411   \n",
      "245        0.022671        0.016783      0.032136        0.042758   \n",
      "246        0.014138        0.012047      0.025543        0.027836   \n",
      "247        0.021481        0.010740      0.027801        0.019040   \n",
      "248        0.024001        0.014720      0.047743        0.025495   \n",
      "249        0.022135        0.013576      0.021860        0.035932   \n",
      "250        0.024970        0.014022      0.043280        0.029821   \n",
      "251        0.020960        0.013100      0.032659        0.028411   \n",
      "252        0.022133        0.016348      0.028024        0.024652   \n",
      "253        0.035030        0.014614      0.034013        0.026219   \n",
      "254        0.018967        0.014109      0.030974        0.015774   \n",
      "255        0.032054        0.024687      0.060217        0.037070   \n",
      "256        0.032121        0.011321      0.022999        0.032541   \n",
      "\n",
      "     ch.9.98989607R  ch.9.991104F  \n",
      "0          0.029481      0.072760  \n",
      "1          0.030634      0.094394  \n",
      "2          0.021008      0.178118  \n",
      "3          0.013007      0.060354  \n",
      "4          0.019746      0.131872  \n",
      "5          0.053140      0.222863  \n",
      "6          0.020539      0.142734  \n",
      "7          0.026503      0.115229  \n",
      "8          0.020598      0.177713  \n",
      "9          0.036511      0.129879  \n",
      "10         0.028690      0.151295  \n",
      "11         0.034512      0.145754  \n",
      "12         0.016535      0.101541  \n",
      "13         0.025116      0.078977  \n",
      "14         0.033821      0.209193  \n",
      "15         0.021871      0.073494  \n",
      "16         0.021440      0.159817  \n",
      "17         0.024134      0.180823  \n",
      "18         0.041102      0.161074  \n",
      "19         0.026198      0.188795  \n",
      "20         0.023477      0.121198  \n",
      "21         0.030533      0.130264  \n",
      "22         0.023002      0.125783  \n",
      "23         0.025573      0.108948  \n",
      "24         0.024743      0.173160  \n",
      "25         0.022830      0.126153  \n",
      "26         0.032968      0.277746  \n",
      "27         0.023240      0.107132  \n",
      "28         0.021588      0.107650  \n",
      "29         0.024745      0.121925  \n",
      "..              ...           ...  \n",
      "227        0.028943      0.159716  \n",
      "228        0.019971      0.106731  \n",
      "229        0.037185      0.241523  \n",
      "230        0.033134      0.105327  \n",
      "231        0.017690      0.087089  \n",
      "232        0.017236      0.150325  \n",
      "233        0.030509      0.113089  \n",
      "234        0.028987      0.111034  \n",
      "235        0.033764      0.198338  \n",
      "236        0.028997      0.222395  \n",
      "237        0.021547      0.133477  \n",
      "238        0.035509      0.227817  \n",
      "239        0.027752      0.030779  \n",
      "240        0.024358      0.134103  \n",
      "241        0.017484      0.097920  \n",
      "242        0.026225      0.148705  \n",
      "243        0.036159      0.175102  \n",
      "244        0.018367      0.153884  \n",
      "245        0.022774      0.101158  \n",
      "246        0.025684      0.109866  \n",
      "247        0.017521      0.095231  \n",
      "248        0.031600      0.162676  \n",
      "249        0.025496      0.037801  \n",
      "250        0.026408      0.073775  \n",
      "251        0.022009      0.117980  \n",
      "252        0.020023      0.093184  \n",
      "253        0.036125      0.111672  \n",
      "254        0.020729      0.142096  \n",
      "255        0.022064      0.177735  \n",
      "256        0.017526      0.085029  \n",
      "\n",
      "[257 rows x 365613 columns]\n",
      "-------Linear SVC ------------\n",
      "[[ 0.06938058  0.26200628  0.91831716 ...,  0.07059141  0.07632897\n",
      "   0.0543123 ]\n",
      " [ 0.08991583  0.14162137  0.04440699 ...,  0.10010196  0.04912976\n",
      "   0.01629672]\n",
      " [ 0.74649849  0.31619652  0.71500661 ...,  0.09675571  0.07422846\n",
      "   0.02133349]\n",
      " ..., \n",
      " [ 0.3055049   0.13927063  0.09758614 ...,  0.09854687  0.04784213\n",
      "   0.0140203 ]\n",
      " [ 0.27472178  0.56578157  0.58791486 ...,  0.05311931  0.07546842\n",
      "   0.05721505]\n",
      " [ 0.52366668  0.71535496  0.19066017 ...,  0.0272784   0.2621927\n",
      "   0.26285902]]\n",
      "0.730769230769\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         3\n",
      "          1       0.43      0.40      0.41        15\n",
      "          2       0.67      0.95      0.78        19\n",
      "          3       0.96      0.87      0.92        31\n",
      "          4       0.00      0.00      0.00         3\n",
      "          5       0.50      0.43      0.46         7\n",
      "\n",
      "avg / total       0.71      0.73      0.71        78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, we'll import pandas, a data processing and CSV file I/O library\n",
    "import pandas as pd\n",
    "from scipy import special\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "#from matplotlib.mlab import PCA as mlabPCA\n",
    "import numpy as np\n",
    "import time\n",
    "#from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import log1p\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "# We'll also import seaborn, a Python graphing library\n",
    "import warnings # current version of seaborn generates a bunch of warnings that we'll ignore\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import hypertools as hyp\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.lda import LDA\n",
    "\n",
    "# The data csv file is given as features as rows and data as columns. \n",
    "# sets IlmnID as index to avoid default indexing. This is needed to avoid another column names while transposing.\n",
    "# Reads all columns now(data)\n",
    "# Reads beta file. Reads manifest. drops rs collumns, sex probes and NA features.\n",
    "def readMedeComReconstructed():\n",
    "    beta_rec_old = pd.read_csv(\"data/reconstructedbeta.csv\", index_col = 0) #, usecols= [*range(0, 261)], index_col = 0')  \n",
    "    #beta = beta_rec_old.T # Transposes to have data along rows\n",
    "    beta_recon = beta_rec_old.transpose().reset_index().rename(columns={'index':'Composite_Element_REF'})\n",
    "    beta_recon.columns.name = None \n",
    "    #Drops columns with 'rs' features from beta file\n",
    "    beta_recon = beta_recon.drop(beta_recon.filter(like='rs').columns, 1)  \n",
    "    print(beta_recon.shape)\n",
    "    #Drops columns with Sex probes(X and Y). Reads from manifestfile\n",
    "    #From manifest sees the column CHR and finds X or Y. If yes, return a list that changed to DF.\n",
    "    # Now make it series data of list.\n",
    "    manifest = pd.read_csv(\"data/HumanMethylation450_15017482_v1-2.csv\")\n",
    "    sex_probes_df = pd.DataFrame(manifest.loc[(manifest['CHR'] == 'X') | (manifest['CHR'] == 'Y')], columns =['IlmnID'])\n",
    "    sex_probes = sex_probes_df['IlmnID'].values.tolist()\n",
    "    beta_recon = beta_recon.drop(beta_recon.filter(sex_probes).columns, 1)\n",
    "    print(beta_recon.shape)\n",
    "    # Drops NaN data if any are NaN (The data has almost all data value NaN for some features)\n",
    "    beta_recon=beta_recon.dropna(axis='rows',how='any')\n",
    "    print(beta_recon.shape)\n",
    "    return beta_recon\n",
    "def readBetaFile():\n",
    "    sarc_old = pd.read_csv(\"data/SARC-combined.txt\", usecols= [*range(0, 261)], index_col = 0, sep='\\t')  \n",
    "    #sarc = sarc_old.T # Transposes to have data along rows\n",
    "    sarc = sarc_old.transpose().reset_index().rename(columns={'index':'Composite_Element_REF'})\n",
    "    sarc.columns.name = None \n",
    "    #print(sarc.shape)\n",
    "    #Drops columns with 'rs' features from beta file\n",
    "    sarc = sarc.drop(sarc.filter(like='rs').columns, 1)  \n",
    "    #print(sarc.shape)\n",
    "    #Drops columns with Sex probes(X and Y). Reads from manifestfile\n",
    "    #From manifest sees the column CHR and finds X or Y. If yes, return a list that changed to DF.\n",
    "    # Now make it series data of list.\n",
    "    manifest = pd.read_csv(\"data/HumanMethylation450_15017482_v1-2.csv\")\n",
    "    sex_probes_df = pd.DataFrame(manifest.loc[(manifest['CHR'] == 'X') | (manifest['CHR'] == 'Y')], columns =['IlmnID'])\n",
    "    sex_probes = sex_probes_df['IlmnID'].values.tolist()\n",
    "    sarc = sarc.drop(sarc.filter(sex_probes).columns, 1)\n",
    "    #print(sarc.shape)\n",
    "    # Drops NaN data if any are NaN (The data has almost all data value NaN for some features)\n",
    "    sarc=sarc.dropna(axis='columns',how='any')\n",
    "    #print(sarc.shape)\n",
    "    return sarc\n",
    "#Reads clinical file and compares it gainst the beta file for Ids to find labels for betas\n",
    "#merges data with corresponding labels\n",
    "#Modifies wrong labels. Drops Despoid tumours.\n",
    "#Drops the outlier\n",
    "def mergeClinicalFile(keepall):\n",
    "    clinic = pd.read_csv(\"data/TCGA_SARC_Clinical-2.csv\", sep='\\s*,\\s*')\n",
    "    #clinic = pd.read_csv(\"dummy.csv\", sep='\\s*,\\s*')\n",
    "    clinic.rename(columns={'Composite Element REF':'Composite_Element_REF'}, inplace=True)\n",
    "    #prints index column calues\n",
    "    #print(clinic.index.values)\n",
    "    #print feature names\n",
    "    #print(list(clinic))\n",
    "    #print index values and corresponding values against the feature - \"Residual Tumor\"\n",
    "    #print(clinic.loc[: , \"residual_tumor\"])\n",
    "    # setting header to the columns of interests\n",
    "    header = [\"Composite_Element_REF\",\"residual_tumor\"]\n",
    "    # saving the index column and residual_columns data to sample.csv\n",
    "    #clinic.to_csv(\"sample.csv\", columns= header)\n",
    "    \n",
    "    # BEAUTIFUL - changes column values instantly, takes fist 10 values, replace . with _ and upper case\n",
    "    merge_clinic = pd.DataFrame(clinic, columns = ['Composite_Element_REF', 'residual_tumor'] )\n",
    "    #merge_clinic.to_csv(\"clinic_ID_label.csv\")\n",
    "    #merge_clinic['Composite_Element_REF'] = merge_clinic['Composite_Element_REF'].apply(lambda x: str(x)[0:12].replace('.', '_').upper())\n",
    "    #print(merge_clinic['Composite_Element_REF'])\n",
    "    #Just to see the data(not needed for project)\n",
    "    print_original = pd.DataFrame(sarcoma, columns = ['Composite_Element_REF'] )\n",
    "    #print_original.to_csv(\"original_beta.csv\")\n",
    "    if(keepall==0):\n",
    "        merge_sarcoma = pd.DataFrame(sarcoma, columns = ['Composite_Element_REF'] )\n",
    "    else:\n",
    "        print(\"HHHHHHH\")\n",
    "        merge_sarcoma = pd.DataFrame(sarcoma)\n",
    "    merge_sarcoma['Composite_Element_REF'] = merge_sarcoma['Composite_Element_REF'].apply(lambda x: str(x)[0:12].replace('-', '.').lower())\n",
    "    #Just to see the data(not needed for project)\n",
    "    analysis_sarc_data = pd.DataFrame(merge_sarcoma, columns = ['Composite_Element_REF'])\n",
    "    #analysis_sarc_data.to_csv(\"betafile_data_Ids.csv\")\n",
    "    #df3 = merge_clinic.merge(merge_sarcoma,on='Composite_Element_REF',\n",
    "                  #how='outer').dropna(subset=['residual_tumor'])\n",
    "    #compares the cloumns for identity and merges sarcoma data set with labels    \n",
    "    result = pd.merge(merge_sarcoma, merge_clinic, on='Composite_Element_REF', how='inner')\n",
    "    # Just for printing to test(not used for project)\n",
    "    analysis_data = pd.DataFrame(result, columns = ['Composite_Element_REF', 'residual_tumor'] )\n",
    "    #analysis_data.to_csv(\"id_label_not_combined.csv\")\n",
    "    #frames = [merge_clinic, merge_sarcoma]\n",
    "    #result = pd.concat(frames)\n",
    "    #result.to_csv(\"sample.csv\")\n",
    "    \n",
    "    result['residual_tumor'] = result.residual_tumor.str.replace(r'(^.*undifferentiated.*$)', 'Undifferentiated Sarcoma')\n",
    "    result['residual_tumor'] = result.residual_tumor.str.replace(r'(^.*synovial.*$)', 'Synovial Sarcoma')\n",
    "    \n",
    "    #Modify Wrong lables in clinical file\n",
    "    result.loc[result['Composite_Element_REF'] == 'tcga.hb.a3yv', 'residual_tumor'] = 'dedifferentiated liposarcoma'\n",
    "    result.loc[result['Composite_Element_REF'] == 'tcga.pc.a5dn', 'residual_tumor'] = 'dedifferentiated liposarcoma'\n",
    "    result.loc[result['Composite_Element_REF'] == 'tcga.dx.a6yz', 'residual_tumor'] = 'dedifferentiated liposarcoma'\n",
    "    result.loc[result['Composite_Element_REF'] == 'tcga.3b.a9hs', 'residual_tumor'] = 'dedifferentiated liposarcoma'\n",
    "    result.loc[result['Composite_Element_REF'] == 'tcga.dx.ab2s', 'residual_tumor'] = 'dedifferentiated liposarcoma'\n",
    "    # Drop Desmoid Tumors\n",
    "    result = result.drop(result.index[result.residual_tumor == 'desmoid tumor']) #tcga.z4.a8jb, tcga.qq.a8vd\n",
    "    result.index = range(len(result)) # Rearrange index\n",
    "    #Delete the outlier    \n",
    "    result = result.drop(result.index[result.Composite_Element_REF == 'tcga.k1.a6rt']) #tcga.k1.a6r\n",
    "    \n",
    "    result.index = range(len(result)) # Rearrange index\n",
    "    # Just for printing to test(not used for project)\n",
    "    analysis_data = pd.DataFrame(result, columns = ['Composite_Element_REF', 'residual_tumor'] )\n",
    "      \n",
    "    #analysis_data.to_csv(\"id_label_combined.csv\")\n",
    "    #Drop ID\n",
    "    result = result.drop('Composite_Element_REF', axis=1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Print values\n",
    "def printValues():\n",
    "    # prints the index column values. Those are Ids related to the data. Not used for calculation                                           \n",
    "    print(sarc.index.values)\n",
    "    print(sarc.index)\n",
    "    # prints the values of data. Index column and column headers are ignored by default. its same as sarc. but an array.\n",
    "    print(sarc.values)\n",
    "    #print feature names except index column\n",
    "    print(list(sarc))\n",
    "# Low variance feature reduction\n",
    "def lowVarianceFeatureReduction():\n",
    "    selector = VarianceThreshold(0.09)\n",
    "    columns = sarcoma_X.columns\n",
    "    sarc_new = selector.fit_transform(sarcoma_X)\n",
    "    column_names = [columns[x] for x in selector.get_support(indices=True) if x]\n",
    "    newdata = pd.DataFrame(selector.fit_transform(sarcoma_X), columns=column_names)\n",
    "    return newdata\n",
    "#PCA with fixed components\n",
    "def pcaWithFixedComp(noOfcomp):\n",
    "    pca = PCA(n_components=noOfcomp)\n",
    "    pca_result = pca.fit_transform(sarcoma_X)    \n",
    "    trainx, testx, trainy, testy = train_test_split(pca_result, sarcoma_Y, test_size=0.3, random_state=0, stratify=sarcoma_Y)\n",
    "    seed = 7\n",
    "    #fit model no training data\n",
    "    model = XGBClassifier()\n",
    "    model.fit(trainx, trainy)\n",
    " \n",
    "    y_test_pred = model.predict(testx)    \n",
    "    cm_tst = confusion_matrix(testy, y_test_pred)\n",
    "    print(cm_tst)\n",
    "    #Plot CM for test data\n",
    "    \n",
    "    plt.clf()\n",
    "    ax = sns.heatmap(cm_tst,annot=True, xticklabels=label_names, yticklabels=label_names)\n",
    "    ax.set_ylabel('Actual classes')\n",
    "    ax.set_xlabel('Predicted classes')\n",
    "    plt.savefig('plots/pca_xgboost_cf.png', bbox_inches='tight')\n",
    "    #Print accuracy report\n",
    "    report = classification_report(testy, y_test_pred)\n",
    "    df = classifaction_report_csv(report)\n",
    "    df.to_csv('plots/pca_xgboost_report.csv', index = True)\n",
    "    print(classification_report(testy, y_test_pred))\n",
    "    print(accuracy_score(testy, y_test_pred))\n",
    "\n",
    "# PCA transformation with TSNE - One practice is use PCA to reduce to 100 then apply tsne to 2 and visualize\n",
    "def pcaWithTSNE():\n",
    "    n_sne = 50\n",
    "    pca = PCA()\n",
    "    pca.fit(sarcoma_X)\n",
    "    cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "    d = np.argmax(cumsum>=0.95) + 1\n",
    "    pca = PCA(n_components = 0.95)\n",
    "    pca_result = pca.fit_transform(sarcoma_X)\n",
    "    print(sum(pca.explained_variance_ratio_))\n",
    "    #TSNE calculation\n",
    "    \n",
    "    for i in [2,3, 4, 5, 7, 8, 10,20,30,40,50,60,70,80,90,100,200]:\n",
    "        X_tsne = TSNE(learning_rate=100, n_components=3, verbose=1, \n",
    "                  perplexity=i, n_iter=10000).fit_transform(pca_result)   \n",
    "        #Visualize tsne and pca\n",
    "        hyp.plot(X_tsne,'o', group=sarcoma_Y, legend=label_names, title = 'TSNE PLOT AFTER PCA PER %s'%i, palette=\"deep\")\n",
    "    #TSNE after Truncated SVD used in sparse data. First redude to less dimensions then tsne.\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    X_reduced = TruncatedSVD(n_components=50, random_state=0).fit_transform(sarcoma_X)\n",
    "    for j in [2,3,4,5,7,10,20,30,40,50,60,70,80,90,100,200]:\n",
    "        X_embedded = TSNE(n_components=3, perplexity=j, verbose=2, n_iter=10000).fit_transform(X_reduced)\n",
    "        hyp.plot(X_embedded,'o', group=sarcoma_Y, legend=label_names, title = 'SVD TSNE PLOT PER %s'%j, palette=\"deep\")\n",
    "    ####################\n",
    "    T_SNE = TSNE(learning_rate=100, n_components=3, verbose=1, \n",
    "                  perplexity=40, n_iter=50000).fit_transform(sarcoma_X)\n",
    "    hyp.plot(T_SNE,'o', group=sarcoma_Y, legend=label_names, title = 'TSNE OVER FULL DIMENSIONS', palette=\"deep\")\n",
    "    ##########################\n",
    "    from sklearn.manifold import MDS\n",
    "    MDS_model = MDS(n_components=3, n_jobs=-1).fit_transform(sarcoma_X)\n",
    "    hyp.plot(T_SNE,'o', group=sarcoma_Y, legend=label_names, title = 'MDS OVER FULL DIMENSIONS', palette=\"deep\")\n",
    "    return pca_result\n",
    "#Correlation matrix\n",
    "def pca_split():\n",
    "    n_sne = 50\n",
    "    pca = PCA()\n",
    "    pca.fit(sarcoma_X)\n",
    "    cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "    d = np.argmax(cumsum>=0.95) + 1\n",
    "    pca = PCA(n_components = 0.95)\n",
    "    pca_result = pca.fit_transform(sarcoma_X)\n",
    "    return pca_result\n",
    "def findCorr(pca_result):\n",
    "    #NumPy Array, convert to Dataframe\n",
    "    corr = pd.DataFrame(pca_result).corr(method='pearson')   \n",
    "    sns.heatmap(corr, \n",
    "        xticklabels=corr.columns.values,\n",
    "        yticklabels=corr.columns.values)\n",
    "    plt.show()\n",
    "#Kernal PCA\n",
    "def logisticRegressionSoftmax(pca_result, result):\n",
    "    print(pca_result)\n",
    "    \n",
    "    trainx, testx, trainy, testy = train_test_split(pca_result, sarcoma_Y, test_size=0.3, random_state=0, stratify=sarcoma_Y)\n",
    "    softmax_reg = linear_model.LogisticRegression(multi_class = \"multinomial\", solver = \"lbfgs\", C = 10)\n",
    "    softmax_reg.fit(trainx, trainy)\n",
    "    y_test_pred = softmax_reg.predict(testx)\n",
    "    accuracy = accuracy_score(testy, y_test_pred)\n",
    "        \n",
    "    cm_tst = confusion_matrix(testy, y_test_pred)\n",
    "    \n",
    "    ax = sns.heatmap(cm_tst,annot=True, xticklabels=label_names, yticklabels=label_names)\n",
    "    ax.set_ylabel('Actual classes')\n",
    "    ax.set_xlabel('Predicted classes')\n",
    "    \n",
    "    plt.savefig('plots/log_reg_cm.png', bbox_inches='tight')\n",
    "    #Print accuracy report\n",
    "    report = classification_report(testy, y_test_pred)\n",
    "    df = classifaction_report_csv(report)\n",
    "    df.to_csv('plots/logregre_report.csv', index = True)\n",
    "    print(accuracy_score(testy, y_test_pred))\n",
    "    print(classification_report(testy, y_test_pred))\n",
    "    \n",
    "def kerPCA():\n",
    "    rbf_pca = KernelPCA(n_components = 2, kernel = \"rbf\", gamma = 0.03)\n",
    "    X_red =  rbf_pca.fit_transform(sarcoma_M_X)\n",
    "    plot_figures(X_red, 'plots/kerPCA.png', \"Kernel PCA\")\n",
    "#printValues()\n",
    "def summaryStats(pca_result):\n",
    "    #pca_result.head() # return the first 5 rows\n",
    "    #pca_result.describe() # summary statistics, excluding NaN values\n",
    "    #pca_result.info(verbose=True, null_counts=True) # concise summary of the table\n",
    "    #pca_result.shape # shape of dataset\n",
    "    print(pca_result.skew()) # skewness for numeric columns\n",
    "    #pca_result.kurt() # unbiased kurtosis for numeric columns\n",
    "    #pca_result.get_dtype_counts() # counts of dtypes\n",
    "    \n",
    "#Encode lables with numerica values and remove the header\n",
    "def encodeLabels(datavalue):\n",
    "    le.fit(datavalue['residual_tumor'])\n",
    "    datavalue['residual_tumor'] = le.transform(datavalue['residual_tumor'])\n",
    "    #labels = datavalue['residual_tumor'] # returns only labels exclusing header\n",
    "    return datavalue\n",
    "def labelBinarize(result):\n",
    "    result = label_binarize(y, classes=[0, 1, 2, 3, 4, 5,6, 7])\n",
    "    \n",
    "\n",
    "def plotCorrTsnePCsnLabels(nComponent):\n",
    "    sns.set(style=\"darkgrid\")\n",
    "    sns.swarmplot(sarcoma_Y,xtsne[:,nComponent],order=[0, 1, 2, 3, 4, 5, 6])\n",
    "    plt.title('Correlation between PCs and target')\n",
    "    plt.xlabel('Target')\n",
    "    plt.ylabel('PC Value')\n",
    "    plt.show()\n",
    "#Plots desnity plot - to see the skewness\n",
    "def plotDensityTsne():\n",
    "    pd.DataFrame(xtsne).plot(kind='density', subplots=True, layout=(3,3), sharex=False)\n",
    "    plt.show()\n",
    "def printXandY():\n",
    "    print(sarcoma_X)\n",
    "    print(sarcoma_Y)\n",
    "#Plots density of mean, mad etc along data(mean and mad is of each category of cancer seperate)\n",
    "def plotDensityAlongDataOrFeat(sarcoma_with_label_codes):\n",
    "    group_by_lables = sarcoma_with_label_codes.groupby(sarcoma_with_label_codes['residual_tumor']).mad()\n",
    "    count_of_labels = sarcoma_with_label_codes[['residual_tumor']].groupby('residual_tumor').size()\n",
    "    group_by_mean = sarcoma_with_label_codes.groupby(sarcoma_with_label_codes['residual_tumor']).mean()\n",
    "    for col_id in range(1000):\n",
    "        #sns.distplot(group_by_mean.iloc[:,col_id]) #Plots along individual features\n",
    "        sns.distplot(group_by_mean.iloc[col_id]) # Plots along data\n",
    "    plt.show()\n",
    "    \n",
    "def tsne_scatter():\n",
    "    \n",
    "    #Pca\n",
    "    pca = PCA()\n",
    "    pca.fit(sarcoma_X)\n",
    "    cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "    d = np.argmax(cumsum>=0.95) + 1\n",
    "    pca = PCA(n_components = 0.95)\n",
    "    pc1 = pca.fit(sarcoma_X)\n",
    "    pca_result = pca.fit_transform(sarcoma_X)\n",
    "    print(pc1.n_components_)\n",
    "    print(pc1.explained_variance_ratio_)\n",
    "    X_tsne = TSNE(learning_rate=100, n_components=2, verbose=1, \n",
    "                  perplexity=60, n_iter=10000).fit_transform(sarcoma_X) \n",
    "    plot_figures(X_tsne, 'plots/tsne.png', 'TSNE(without PCA)')\n",
    "    \n",
    "def classify_LDA(no_of_components, type_of_cancer_label):\n",
    "    lda = LinearDiscriminantAnalysis(n_components=no_of_components)\n",
    "    lda_transformed = lda.fit_transform(sarcoma_X, sarcoma_Y)\n",
    "    #print(lda_transformed)\n",
    "    #lda_csv = pd.DataFrame(lda_transformed)\n",
    "    #lda_csv.to_csv(\"lda_csv.csv\")\n",
    "    #y_csv = pd.DataFrame(sarcoma_Y)\n",
    "    #y_csv.to_csv(\"y_csv.csv\")\n",
    "    #print(type(sarcoma_Y))\n",
    "    #print(y_nump)\n",
    "    #hyp.plot(lda_transformed,'o', legend=label_names, title = 'LDA OVER FULL DIMENSIONS', palette=\"deep\")\n",
    "    #plt.scatter(lda_transformed[:,0], lda_transformed[:,1], c=y,cmap=plt.cm.coolwarm, label = label_names)\n",
    "    #plt.legend(loc='best', shadow=False, scatterpoints=None)\n",
    "    plot_figures(lda_transformed, 'plots/lda.png', \"LDA\")\n",
    "def classify_LDA_Test():\n",
    "    lda = LinearDiscriminantAnalysis(n_components=2) # Creating LDA object\n",
    "    lda = lda.fit(X_train, y_train) # Create the transformation matrix\n",
    "    lda_train = lda.transform(X_train) # Transforms X train data\n",
    "    plt.clf()\n",
    "    colors = ['magenta', 'cyan', 'grey', 'red', 'green', 'orange']\n",
    "    fig = plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    y_nump =y_train.values\n",
    "    title = \"LDA TRAIN DATA\"\n",
    "    fileName = \"lda_split_train.png\"\n",
    "    for color, i, target_name in zip(colors, [0, 1, 2, 3, 4, 5], label_names):\n",
    "        ax.scatter(lda_train[y_nump == i, 0], lda_train[y_nump == i, 1], alpha=.8, color=color,label=target_name)\n",
    "    \n",
    "    # Shrink current axis by 20%\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "    lgd = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5)) \n",
    "    ax.grid('on')\n",
    "    plt.title('%s plot of Sarcoma Dataset'%title)\n",
    "    plt.savefig(fileName, bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "    plt.show()\n",
    "    test_labels = lda.predict(X_test)\n",
    "    plt.clf()\n",
    "    colors = ['magenta', 'cyan', 'grey', 'red', 'green', 'orange']\n",
    "    fig = plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    y_nump =test_labels.values\n",
    "    title = \"LDA TEST DATA\"\n",
    "    fileName = \"lda_split_test.png\"\n",
    "    for color, i, target_name in zip(colors, [0, 1, 2, 3, 4, 5], label_names):\n",
    "        ax.scatter(lda_test[y_nump == i, 0], lda_test[y_nump == i, 1], alpha=.8, color=color,label=target_name)\n",
    "    \n",
    "    # Shrink current axis by 20%\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "    lgd = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5)) \n",
    "    ax.grid('on')\n",
    "    plt.title('%s plot of Sarcoma Dataset'%title)\n",
    "    plt.savefig(fileName, bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "    plt.show() \n",
    "       \n",
    "    cm_tst = confusion_matrix(y_test, test_labels)\n",
    "    print(cm_tst)\n",
    "    #Plot CM for test data\n",
    "    ax = sns.heatmap(cm_tst,annot=True, xticklabels=label_names, yticklabels=label_names)\n",
    "    ax.set_ylabel('Actual classes')\n",
    "    ax.set_xlabel('Predicted classes')\n",
    "    \n",
    "    plt.savefig('plots/lda_test_CM.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "def classifaction_report_csv(report):\n",
    "    report_data = []\n",
    "    lines = report.split('\\n')\n",
    "    for line in lines[2:-3]:\n",
    "        row = {}\n",
    "        row_data = line.split('      ')\n",
    "        row['cancer type'] = row_data[1]\n",
    "        row['precision'] = float(row_data[2])\n",
    "        row['recall'] = float(row_data[3])\n",
    "        row['f1_score'] = float(row_data[4])\n",
    "        row['support'] = float(row_data[5])\n",
    "        report_data.append(row)\n",
    "    dataframe = pd.DataFrame.from_dict(report_data)\n",
    "    return dataframe\n",
    "def plot_figures(data_to_plot, fileName, title):\n",
    "    #Plot params\n",
    "    plt.clf()\n",
    "    colors = ['magenta', 'cyan', 'grey', 'red', 'green', 'orange']\n",
    "    fig = plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    y_nump =sarcoma_Y.values\n",
    "    for color, i, target_name in zip(colors, [0, 1, 2, 3, 4, 5], label_names):\n",
    "        ax.scatter(data_to_plot[y_nump == i, 0], data_to_plot[y_nump == i, 1], alpha=.8, color=color,label=target_name)\n",
    "    \n",
    "    # Shrink current axis by 20%\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "    lgd = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5)) \n",
    "    ax.grid('on')\n",
    "    plt.title('%s plot of Sarcoma Dataset'%title)\n",
    "    plt.savefig(fileName, bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "def classify_linearSVC():\n",
    "    #Find hyperparameter for Linear SVC based on accuracy\n",
    "    \n",
    "    svc = LinearSVC(penalty=\"l1\", dual=False) # trains 6 models of shape(_samples, n_classes)\n",
    "    cv = StratifiedKFold(n_splits=5)\n",
    "    C_s = np.logspace(-3, 1, 30)  #10^-3 to 10^ 1 with 10 steps. so its 0.001 to 1 \n",
    "    scores = list()\n",
    "    scores_std = list()\n",
    "    for C in C_s:\n",
    "        svc.C = C\n",
    "        #cross validation accuracy. \n",
    "        this_scores = cross_val_score(svc, X_train, y_train, cv = 4, n_jobs=1)\n",
    "        scores.append(np.mean(this_scores))\n",
    "        scores_std.append(np.std(this_scores))\n",
    "    # Do the plotting\n",
    "    plt.figure(1, figsize=(4, 3))\n",
    "    #plt.clf()\n",
    "    plt.semilogx(C_s, scores)\n",
    "    plt.semilogx(C_s, np.array(scores) + np.array(scores_std), 'b--')\n",
    "    plt.semilogx(C_s, np.array(scores) - np.array(scores_std), 'b--')\n",
    "    locs, labels = plt.yticks()\n",
    "    plt.yticks(locs, list(map(lambda x: \"%g\" % x, locs)))\n",
    "    plt.ylabel('CV score')\n",
    "    plt.xlabel('Parameter C')\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.savefig('plots/linearsvc_CV_error.png')\n",
    "    plt.show()\n",
    "    # Another way to meaure accuracy of training with cross validation, but here we took C = best. \n",
    "    # we can do the same above to find C\n",
    "    \n",
    "    plt.clf()\n",
    "    best_C = C_s[np.argmax(scores)]\n",
    "    #Just check the prediction efficiency against best C for 5 fold cross validated data\n",
    "    svc.C = best_C\n",
    "    y_train_pred = cross_val_predict(svc,X_train,y_train, cv=4)   \n",
    "    cm = confusion_matrix(y_train, y_train_pred)\n",
    "    # Plot confusion metrix against train prediction cv   \n",
    "    ax = sns.heatmap(cm,annot=True, xticklabels=label_names, yticklabels=label_names)\n",
    "    ax.set_ylabel('Actual classes')\n",
    "    ax.set_xlabel('Predicted classes')\n",
    "    \n",
    "    plt.savefig('plots/linearSVC_train.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    np.fill_diagonal(cm, 0)\n",
    "    ax = sns.heatmap(cm,annot=True, xticklabels=label_names, yticklabels=label_names)\n",
    "    ax.set_ylabel('Actual classes')\n",
    "    ax.set_xlabel('Predicted classes')\n",
    "    \n",
    "    plt.savefig('plots/linearSVC_train_filldiag.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    #Fit model to the train data with best C value\n",
    "    svc.fit(X_train, y_train)\n",
    "    y_test_pred = svc.predict(X_test)\n",
    "    cm_tst = confusion_matrix(y_test, y_test_pred)\n",
    "    print(cm_tst)\n",
    "    #Plot CM for test data\n",
    "    ax = sns.heatmap(cm_tst,annot=True, xticklabels=label_names, yticklabels=label_names)\n",
    "    ax.set_ylabel('Actual classes')\n",
    "    ax.set_xlabel('Predicted classes')\n",
    "    \n",
    "    plt.savefig('plots/linearSVC_test.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    np.fill_diagonal(cm_tst, 0)\n",
    "    ax = sns.heatmap(cm_tst,annot=True, xticklabels=label_names, yticklabels=label_names)\n",
    "    ax.set_ylabel('Actual classes')\n",
    "    ax.set_xlabel('Predicted classes')\n",
    "    \n",
    "    plt.savefig('plots/linearSVC_test_filldiag.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    #Print accuracy report\n",
    "    report = classification_report(y_test, y_test_pred)\n",
    "    df = classifaction_report_csv(report)\n",
    "    df.to_csv('plots/linearSVC_report.csv', index = True)\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    print(accuracy_score(y_test, y_test_pred))\n",
    "    \n",
    "    #cm_perf = confusion_matrix(y_train, y_train_perfect_predictions)\n",
    "def classify_NonlinearSVC():\n",
    "    #lsvc = LinearSVC(C=1000, penalty=\"l1\", dual=False).fit(sarcoma_X, sarcoma_Y)\n",
    "    #model = SelectFromModel(lsvc, prefit=True)\n",
    "    #X_new = model.transform(sarcoma_X)\n",
    "    #print(X_new.shape)\n",
    "    svc = SVC(C=1000, kernel=\"rbf\")\n",
    "    n_samples = X_train.shape[0]\n",
    "    cv = ShuffleSplit(n_samples, n_iter=3, test_size=0.3, random_state=0)\n",
    "    score = cross_val_score(svc, X_train, y_train, cv=cv)\n",
    "    print(score)\n",
    "def classify_linearSVC_CV_autom():\n",
    "    #--Split into Train and test set 80/20\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(sarcoma_X, sarcoma_Y, test_size=0.2, random_state=0, stratify=sarcoma_Y)\n",
    "    #--Choose estimator\n",
    "    estimator = SVC(kernel=\"rbf\")\n",
    "    #--Choose Cross validation estimator\n",
    "    cv = ShuffleSplit(X_train.shape[0], n_iter=10, test_size=0.2, random_state=0)\n",
    "    #-- Tune Hyperparameters\n",
    "    gammas = np.logspace(-6, -1, 10)\n",
    "    classifier = GridSearchCV(estimator=estimator, cv=cv, param_grid=dict(gamma=gammas))\n",
    "    classifier.fit(X_train, y_train)\n",
    "    #-- Debug algorithm with learning curve\n",
    "    title = 'Learning Curves (SVM, linear kernel, $\\gamma=%.6f$)' %classifier.best_estimator_.gamma\n",
    "    estimator = SVC(kernel='rbf', gamma=classifier.best_estimator_.gamma)\n",
    "    plot_learning_curve(estimator, title, X_train, y_train, cv=cv)\n",
    "    plt.show()\n",
    "    #pred = lsvc.predict(X_test)\n",
    "    \n",
    "def classifyXGBoost():\n",
    "    seed = 7\n",
    "    #fit model no training data\n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    cm = confusion_matrix(y_train, y_train_pred)\n",
    "    # Plot confusion metrix against train prediction cv   \n",
    "    ax = sns.heatmap(cm,annot=True, xticklabels=label_names, yticklabels=label_names)\n",
    "    ax.set_ylabel('Actual classes')\n",
    "    ax.set_xlabel('Predicted classes')\n",
    "    plt.savefig('plots/xgboost_train_cf.png', bbox_inches='tight')\n",
    "    \n",
    "    plt.clf()\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    predictions = [round(value) for value in y_test_pred]\n",
    "    #evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))    \n",
    "    cm_tst = confusion_matrix(y_test, y_test_pred)\n",
    "    #Plot CM for test data\n",
    "    \n",
    "    ax = sns.heatmap(cm_tst,annot=True, xticklabels=label_names, yticklabels=label_names)\n",
    "    ax.set_ylabel('Actual classes')\n",
    "    ax.set_xlabel('Predicted classes')\n",
    "    \n",
    "    plt.savefig('plots/xgboost_test_cf.png', bbox_inches='tight')\n",
    "    #Print accuracy report\n",
    "    report = classification_report(y_test, y_test_pred)\n",
    "    df = classifaction_report_csv(report)\n",
    "    df.to_csv('plots/xgboost_report.csv', index = True)\n",
    "    print(accuracy_score(y_test, y_test_pred))\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "# Does cross validation, takes parameter tuning, use SVC or lR based on loss etc\n",
    "def classify_linearSGD():\n",
    "    tuned_parameters = {'alpha': [10 ** a for a in range(-4, -2)], 'l1_ratio' : [1 * a for a in range(0, 1)] }\n",
    "    # Important for tunedparameters in grdsearch\n",
    "    #tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "    #                 'C': [1, 10, 100, 1000]},\n",
    "    #                {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "    #SGDclassifier with loss = 'hinge' means its SVC  if loss = 'log' its logistic classfier\n",
    "    clf = GridSearchCV(SGDClassifier(loss='hinge', penalty='elasticnet',\n",
    "                    n_iter=400, \n",
    "                    shuffle=True, verbose=False, n_jobs=10, average=False, class_weight='balanced'),\n",
    "                    tuned_parameters, cv=10, scoring='f1_macro')\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(clf.best_params_) \n",
    "def classify_RandomForestClassifier():\n",
    "    #rfc = RandomForestClassifier(n_jobs=-1,max_features= 'sqrt' ,n_estimators=1000, oob_score = True)\n",
    "    #param_grid = { \n",
    "        #'n_estimators': [5000],\n",
    "        #'max_features': ['auto', 'sqrt', 'log2']\n",
    "    #}\n",
    "    #CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "    #CV_rfc.fit(lowVSar_X, sarcoma_Y)\n",
    "    #print(CV_rfc.best_params_)\n",
    "    #print(CV_rfc.best_score_)\n",
    "    \n",
    "    '''rfc1 = RandomForestClassifier(oob_score = True)\n",
    "    rfc1.fit(X_train, y_train)\n",
    "    param_grid = { \"n_estimators\"      : [4000, 5000, 6000, 7000, 8000],\n",
    "           \"criterion\"         : [\"gini\", \"entropy\"],\n",
    "           \"max_features\"      : [10, 20, 30, 40, 50],\n",
    "           \"max_depth\"         : [10, 20, 30, 40, 50],\n",
    "           \"min_samples_split\" : [10, 20, 30, 40] ,\n",
    "           \"bootstrap\": [True, False]}\n",
    "    grid_search = GridSearchCV(rfc1, param_grid, n_jobs=-1, cv=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(grid_search.best_params_)'''\n",
    "    \n",
    "    \n",
    "    #print(list(zip(sarcoma_X, clf.feature_importances_)))\n",
    "    #importances = forest.feature_importances_\n",
    "    \n",
    "    #X_train, X_test, y_train, y_test = train_test_split(features, sarcoma_Y, test_size=0.3, random_state=0, stratify=sarcoma_Y)\n",
    "    rfc = RandomForestClassifier(n_jobs=-1,max_features= 'auto' ,n_estimators=8000, oob_score = True)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = rfc.predict(X_train)\n",
    "    cm = confusion_matrix(y_train, y_train_pred)\n",
    "    # Plot confusion metrix against train prediction cv   \n",
    "    ax = sns.heatmap(cm,annot=True, xticklabels=label_names, yticklabels=label_names)\n",
    "    ax.set_ylabel('Actual classes')\n",
    "    ax.set_xlabel('Predicted classes')\n",
    "    plt.savefig('plots/rf_train_cf.png', bbox_inches='tight')\n",
    "       \n",
    "    y_test_pred = rfc.predict(X_test)    \n",
    "    cm_tst = confusion_matrix(y_test, y_test_pred)\n",
    "    #print(cm_tst)\n",
    "    #Plot CM for test data\n",
    "    plt.clf()\n",
    "    ax = sns.heatmap(cm_tst,annot=True, xticklabels=label_names, yticklabels=label_names)\n",
    "    ax.set_ylabel('Actual classes')\n",
    "    ax.set_xlabel('Predicted classes')\n",
    "    plt.savefig('plots/rf_test_cf.png', bbox_inches='tight')\n",
    "    #Print accuracy report\n",
    "    report = classification_report(y_test, y_test_pred)\n",
    "    df = classifaction_report_csv(report)\n",
    "    df.to_csv('plots/rf_report.csv', index = True)\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    print(accuracy_score(y_test, y_test_pred))\n",
    "def test():\n",
    "    forest = RandomForestClassifier(n_estimators=1000)\n",
    "    forest.fit(X_train, y_train)\n",
    "    #print(list(zip(sarcoma_X, clf.feature_importances_)))\n",
    "    importances = forest.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "\n",
    "    for f in range(X_train.shape[1]):\n",
    "        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure()\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(X_train.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "    plt.xticks(range(X_train.shape[1]), indices)\n",
    "    plt.xlim([-1, X_train.shape[1]])\n",
    "    plt.show()\n",
    "def Univariate_ChiSquare_XGBoost():\n",
    "    test = SelectKBest(score_func=chi2, k=130000)\n",
    "    \n",
    "    fit = test.fit(sarcoma_X, sarcoma_Y)\n",
    "    #numpy.set_printoptions(precision=3)\n",
    "    #print(fit.scores_)\n",
    "    features = fit.transform(sarcoma_X)\n",
    "    print(features.shape)\n",
    "    \n",
    "    seed = 7\n",
    "    \n",
    "    trainx, testx, trainy, testy = train_test_split(features, sarcoma_Y, test_size=0.3, random_state=0, stratify=sarcoma_Y)\n",
    "    \n",
    "    #fit model no training data\n",
    "    model = XGBClassifier()\n",
    "    model.fit(trainx, trainy)\n",
    "    \n",
    "    y_test_pred = model.predict(testx)    \n",
    "    cm_tst = confusion_matrix(testy, y_test_pred)\n",
    "    print(cm_tst)\n",
    "    #Plot CM for test data\n",
    "    \n",
    "    plt.clf()\n",
    "    ax = sns.heatmap(cm_tst,annot=True, xticklabels=label_names, yticklabels=label_names)\n",
    "    ax.set_ylabel('Actual classes')\n",
    "    ax.set_xlabel('Predicted classes')\n",
    "    plt.savefig('chisquare_test_cf.png', bbox_inches='tight')\n",
    "    #Print accuracy report\n",
    "    report = classification_report(testy, y_test_pred)\n",
    "    df = classifaction_report_csv(report)\n",
    "    df.to_csv('chsq_report.csv', index = True)\n",
    "    print(classification_report(testy, y_test_pred))\n",
    "    print(accuracy_score(testy, y_test_pred))\n",
    "    \n",
    "def getLabelCounts(labels):\n",
    "    df = pd.DataFrame(labels)\n",
    "    print(df.residual_tumor.value_counts())\n",
    "    \n",
    "def Univariate_ChiSquare():\n",
    "    chi_square = SelectKBest(score_func=chi2, k=20000)    \n",
    "    fit = chi_square.fit(sarcoma_X, sarcoma_Y)\n",
    "    features = fit.transform(sarcoma_X)\n",
    "    return features\n",
    "def classify_RF_PCA(noOfcomp):\n",
    "   \n",
    "    pca = PCA(n_components=noOfcomp)\n",
    "    pca_result = pca.fit_transform(sarcoma_X)\n",
    "    \n",
    "    trainx, testx, trainy, testy = train_test_split(pca_result, sarcoma_Y, test_size=0.3, random_state=0, stratify=sarcoma_Y)\n",
    "    \n",
    "    seed = 7\n",
    "    rfc = RandomForestClassifier(n_jobs=-1,max_features= 'auto' ,n_estimators=8000, oob_score = True)\n",
    "    rfc.fit(trainx, trainy)\n",
    "    \n",
    "    \n",
    "    y_test_pred = rfc.predict(testx)    \n",
    "    cm_tst = confusion_matrix(testy, y_test_pred)\n",
    "    #print(cm_tst)\n",
    "    #Plot CM for test data\n",
    "    plt.clf()\n",
    "    ax = sns.heatmap(cm_tst,annot=True, xticklabels=label_names, yticklabels=label_names)\n",
    "    ax.set_ylabel('Actual classes')\n",
    "    ax.set_xlabel('Predicted classes')\n",
    "    plt.savefig('plots/pca_rf_test_cf.png', bbox_inches='tight')\n",
    "    #Print accuracy report\n",
    "    report = classification_report(testy, y_test_pred)\n",
    "    df = classifaction_report_csv(report)\n",
    "    df.to_csv('plots/pca_rf_report.csv', index = True)\n",
    "    print(classification_report(testy, y_test_pred))\n",
    "    print(accuracy_score(testy, y_test_pred))\n",
    "#Mislabels data in clinical file need to be changed\n",
    "    \n",
    "    \n",
    "# Function calls starts . Reads beta file.\n",
    "sarcoma = readBetaFile()\n",
    "#Reads reconstructed MedCom file\n",
    "#sarcoma = readMedeComReconstructed()\n",
    "#Result df contains only label data if parameter passed is 0. else it returns a df with all merged with labels\n",
    "# features names, index, label values. But not the Composite_Element_REF. Its dropped.\n",
    "sarcoma_with_label_names = mergeClinicalFile(1)\n",
    "#Use only for MedeCom file\n",
    "#sarcoma_with_label_names.to_csv(\"reconstr_beta_withlabel.csv\")\n",
    "#print(sarcoma_with_label_names.shape)\n",
    "#print(getLabelCounts(sarcoma_with_label_names)) # Good one to show no of labels for each\n",
    "#Returns encoded same as result one. with encoded labels. Would say this is the final data \n",
    "#with features and labels headings as well.\n",
    "le = LabelEncoder()\n",
    "sarcoma_with_label_codes = encodeLabels(sarcoma_with_label_names)\n",
    "#print(sarcoma_with_label_codes.shape)\n",
    "label_names = list(le.inverse_transform([0, 1, 2, 3, 4, 5]))\n",
    "print(label_names)\n",
    "\n",
    "sarcoma_Y= sarcoma_with_label_codes['residual_tumor']\n",
    "\n",
    "sarcoma_X = sarcoma_with_label_codes.drop('residual_tumor', axis=1)\n",
    "\n",
    "sarcoma_X_scale = scale(sarcoma_X)\n",
    "sarcoma_M_X = special.logit(sarcoma_X)\n",
    "print(sarcoma_X)\n",
    "#Feature select with variance\n",
    "#pca_result = pca_split() # Use with LogisticRegressionSoftmax()\n",
    "features = Univariate_ChiSquare() # Use with LogisticRegressionSoftmax()\n",
    "#features = lowVarianceFeatureReduction() # Use with LogisticRegressionSoftmax()\n",
    "X_train, X_test, y_train, y_test = train_test_split(sarcoma_X, sarcoma_Y, test_size=0.3, random_state=0, stratify=sarcoma_Y)\n",
    "#Use only when we use feature selection algorithms\n",
    "#X_train, X_test, y_train, y_test = train_test_split(features, sarcoma_Y, test_size=0.3, random_state=0, stratify=sarcoma_Y)\n",
    "#print(X_train)\n",
    "#lowVSar_X = lowVarianceFeatureReduction()\n",
    "#print(\"Full Set of Labels below ******************\")\n",
    "#getLabelCounts(sarcoma_Y)\n",
    "#print(\"Train Set of Labels below ******************\")\n",
    "#getLabelCounts(y_test) # Print label counts\n",
    "#getLabelCounts(y_train) # Print label counts\n",
    "#print(\"Test Set of Labels below ******************\")\n",
    "#getLabelCounts(y_test)\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(lowVSar_X, sarcoma_Y, test_size=0.30, random_state=0, stratify=sarcoma_Y)\n",
    "\n",
    "#printXandY()\n",
    "\n",
    "#pcaWithFixedComp(10000)\n",
    "#summaryStats(pd.DataFrame(pca_result))\n",
    "############\n",
    "\n",
    "#plotDensityTsne()\n",
    "###############\n",
    "#plotCorrTsnePCsnLabels(0)\n",
    "#plotCorrTsnePCsnLabels(1)\n",
    "#plotCorrTsnePCsnLabels(2)\n",
    "#sns.violinplot(sarcoma_data.iloc[0], color = 'cyan')\n",
    "#plt.show()\n",
    "#findCorr(pca_result) \n",
    "#plotDensityAlongDataOrFeat(sarcoma_with_label_codes)\n",
    "#classify_RF_PCA(10000)\n",
    "#classify_LDA_Test()\n",
    "####################### Classifiers #######################\n",
    "#kerPCA()\n",
    "#classify_LDA(2, 7) # 2 is no of components, 7 means plots all labels else plots specific type of cancer only\n",
    "#tsne_scatter()\n",
    "print(\"-------Linear SVC ------------\")\n",
    "#classify_linearSVC()# 71.25 with medecom\n",
    "#print(\"-------RF ------------\")\n",
    "#classify_RandomForestClassifier() #77.5% with medecom and n_estimators 8000(>5000)\n",
    "#print(\"-------XG Boost ------------\")\n",
    "#classifyXGBoost() #  72.5 with medecom\n",
    "\n",
    "#classify_linearSGD()\n",
    "#classify_NonlinearSVC()\n",
    "#########################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#test()\n",
    "#######################\n",
    "logisticRegressionSoftmax(features, sarcoma_Y)\n",
    "##### FEATURE SELECTION - RF Classifier\n",
    "\n",
    "\n",
    "\n",
    "#svc = SVC(kernel=\"linear\")\n",
    "#rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2),\n",
    "              #scoring='accuracy')\n",
    "#rfecv.fit(X_train, y_train)\n",
    "\n",
    "#print(rfecv.n_features_)\n",
    "\n",
    "#Plot number of features VS. cross-validation scores\n",
    "#plt.figure()\n",
    "#plt.xlabel(\"Number of features selected\")\n",
    "#plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "#plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
